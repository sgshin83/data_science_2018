{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Applying Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426,)\n",
      "\n",
      "(143, 30)\n",
      "(143,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스케일링을 수행할 scaler에게 fit 함수를 호출함. 이때 훈련 데이터만 넘겨줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제로 훈련 데이터의 스케일을 조정하려면 scaler의 transform 메소드를 호출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### axis = 0, colum, axis = 1law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "per-feature minimum before scaling:\n",
      " [  6.98100000e+00   9.71000000e+00   4.37900000e+01   1.43500000e+02\n",
      "   5.26300000e-02   1.93800000e-02   0.00000000e+00   0.00000000e+00\n",
      "   1.06000000e-01   5.02400000e-02   1.15300000e-01   3.60200000e-01\n",
      "   7.57000000e-01   6.80200000e+00   1.71300000e-03   2.25200000e-03\n",
      "   0.00000000e+00   0.00000000e+00   9.53900000e-03   8.94800000e-04\n",
      "   7.93000000e+00   1.20200000e+01   5.04100000e+01   1.85200000e+02\n",
      "   7.11700000e-02   2.72900000e-02   0.00000000e+00   0.00000000e+00\n",
      "   1.56600000e-01   5.52100000e-02]\n",
      "per-feature maximum before scaling:\n",
      " [  2.81100000e+01   3.92800000e+01   1.88500000e+02   2.50100000e+03\n",
      "   1.63400000e-01   2.86700000e-01   4.26800000e-01   2.01200000e-01\n",
      "   3.04000000e-01   9.57500000e-02   2.87300000e+00   4.88500000e+00\n",
      "   2.19800000e+01   5.42200000e+02   3.11300000e-02   1.35400000e-01\n",
      "   3.96000000e-01   5.27900000e-02   6.14600000e-02   2.98400000e-02\n",
      "   3.60400000e+01   4.95400000e+01   2.51200000e+02   4.25400000e+03\n",
      "   2.22600000e-01   9.37900000e-01   1.17000000e+00   2.91000000e-01\n",
      "   5.77400000e-01   1.48600000e-01]\n",
      "per-feature minimum after scaling:\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "per-feature maximum after scaling:\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "\n",
    "# axis=0 --> 426개의 데이터들에 대해 동일한 Colume에 속한 각 특성값들에 대해 MinMaxScaling을 수행함\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0))) \n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min = 0, max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 데이터도 변환해줌\n",
    "  - [주의] 테스트 데이터 변환시에 항상 훈련 데이터들의 통계치만을 사용\n",
    "  - 즉, 테스트 데이터에 대해 다음과 같은 공식을 통하여 변환\n",
    "$$\\dfrac{x_{test_{ij}} – x_{train_{min_j}}}{x_{train_{max_j}} – x_{train_{min_j}}}$$\n",
    "  - 위 식에서 i는 각 데이터 인덱스, j는 각 데이터들에 대한 특성 인덱스. 즉, min_j와 max_j는 동일한 j번째 특성들 전체에 대한 최소 및 최대값을 의미 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-feature minimum after scaling:\n",
      "[ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704\n",
      "  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501\n",
      "  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.\n",
      " -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767\n",
      "  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]\n",
      "per-feature maximum after scaling:\n",
      "[ 0.9578778   0.81501522  0.95577362  0.89353128  0.81132075  1.21958701\n",
      "  0.87956888  0.9333996   0.93232323  1.0371347   0.42669616  0.49765736\n",
      "  0.44117231  0.28371044  0.48703131  0.73863671  0.76717172  0.62928585\n",
      "  1.33685792  0.39057253  0.89612238  0.79317697  0.84859804  0.74488793\n",
      "  0.9154725   1.13188961  1.07008547  0.92371134  1.20532319  1.63068851]\n"
     ]
    }
   ],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x_train data를 scaling 한 데이터를 만듬( fit ,  transform)\n",
    "#### 실제 새로운 입력 데이터는 min, max를 모르기 떄문에, fit 를 할 수 업기 때문에 훈련 데이터로 수행해야함\n",
    "#### 그래서 0보다 작거나 1보다 큰 scale 값이 생길 수 있음\n",
    "#### gradient boosting classifier.fit( x_train_scaled, y_train) 활용\n",
    "#### 이후 GBC.predict, GBC.score 함수 사용해서 비지도학습 결과 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Scaling training and test data the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-038b7367f8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# plot the training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training set\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'^'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Test set\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# split it into training and test sets\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# plot the training and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^', c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^', c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "\n",
    "# rescale the test set separately\n",
    "# so test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 데이터만 가지고 fit 하고 test 데이터에 적용\n",
    "#### Test 데이터만을 위해 별도의 scaler 를 따로 만들어서 fitting을 한 예를 보이며, 이렇게 하지 말라고 얘기함( 이렇게 해서 3번 그래프의 test 값에 변화가 생김)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 첫번째 그래프와 두번째 그래프는 축의 눈금이 변한 것을 제외하면 동일함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[단축 메소드 사용]\n",
    "- fit --> transform\n",
    "- fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# calling fit and transform in sequence (using method chaining)\n",
    "X_scaled = scaler.fit(X_train).transform(X_train)\n",
    "# same result, but more efficient computation\n",
    "X_scaled_d = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 The effect of preprocessing on supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 3.3.4 The effect of preprocessing on supervised learning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "•C: 규제의 강도를 결정하는 매개변수◾높은 C 값: 규제의 감소 --> 훈련 세트에 최대로 맞춤\n",
    "◾낮은 C 값: 규제의 증대 --> 계수 백터(w)를 최대로 0에 가깝게 만듦\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C 가 100이면 큰 편이며, 이는 규제를 별로 하지 않음 -> 오버핏 형태 가능\n",
    "- MinMaxScaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(\"Scaled test set accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler 사용\n",
    "- min max scaler 만듬\n",
    "- scaled 된 입력 데이터를 통해 정확도가 많이 상승됨\n",
    "** scale 을 했을 때, 정확도가 높아지는데, 반대 상황이 있나? 그 기준은 뭔가가 퀘스쳔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
